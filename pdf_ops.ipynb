{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessionalPallav20014/RAG-Based-AI-PDF-Summarizer/blob/main/pdf_ops.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7402c3a"
      },
      "source": [
        "This cell installs the essential Python libraries needed for our RAG (Retrieval Augmented Generation) system. These include `langchain` for building LLM applications, `pypdf` for reading PDF files, `sentence-transformers` for creating text embeddings, `faiss-cpu` for efficient similarity search, and `transformers` for working with pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIb_OaK2y7JT"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community pypdf sentence-transformers faiss-cpu transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c6d2d10"
      },
      "source": [
        "This cell installs or updates `langchain-text-splitters`, a library used to break down large documents into smaller, more digestible pieces. This is important for the RAG system to process information effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVQuP0bLz4W_"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-text-splitters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71514f23"
      },
      "source": [
        "This cell installs `langchain-classic`, which provides components for building applications with large language models, including the `RetrievalQA` chain used in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTgsOFt_11-k"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-classic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad439999"
      },
      "source": [
        "This cell installs `Gradio`, a library that helps us create a simple and interactive web interface for our RAG system, so we can easily ask questions and get answers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ed86e93"
      },
      "source": [
        "# Install Gradio\n",
        "!pip install gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d9af3a"
      },
      "source": [
        "This cell imports all the important tools (Python libraries) we'll need for our RAG system, like `os` for interacting with the operating system, `logging` to see what's happening behind the scenes, and various components from `langchain` and `transformers` to handle documents, embeddings, and the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9I_Y5PnzNVL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from google.colab import files\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8df788d"
      },
      "source": [
        "This cell defines a special blueprint called `LocalRAGSystem`. Think of it as a master plan for building our question-answering robot. It includes all the steps like uploading documents, splitting them, understanding their meaning, and finally answering questions using a local language model.\n",
        "\n",
        "This RAG model is not like the complicated or accurate one and it's made for the purpose of running the model locally without pulling any big model with billions of parameters, which can work better but they will occupy a lot of disk space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acxuDci72inc"
      },
      "outputs": [],
      "source": [
        "# to keep things organized\n",
        "class LocalRAGSystem:\n",
        "  def __init__(self):\n",
        "    self.documents=[]\n",
        "    self.vector_store=None\n",
        "    self.embeddings=None\n",
        "    self.llm=None\n",
        "    self.qa_chain=None\n",
        "\n",
        "  # to upload the pdf dynamically in google colab\n",
        "  def upload_pdfs(self):\n",
        "    uploaded=files.upload()\n",
        "    pdf_paths=list(uploaded.keys())\n",
        "    logger.info(f\"Uploaded PDFs: {pdf_paths}\")\n",
        "    return pdf_paths\n",
        "\n",
        "  # to load and parse PDF documents\n",
        "  def load_documents(self, pdf_paths):\n",
        "      for pdf_path in pdf_paths:\n",
        "          loader = PyPDFLoader(pdf_path)\n",
        "          documents = loader.load()\n",
        "          self.documents.extend(documents)\n",
        "      logger.info(f\"Loaded {len(self.documents)} pages in total.\")\n",
        "\n",
        "  def split_documents(self, chunk_size=1000, chunk_overlap=200):\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "      self.document_chunks = text_splitter.split_documents(self.documents)\n",
        "      logger.info(f\"Split into {len(self.document_chunks)} chunks.\")\n",
        "\n",
        "  def setup_embeddings(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "      self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "      logger.info(f\"Embedding model {model_name} loaded.\")\n",
        "\n",
        "  def create_vector_store(self):\n",
        "      self.vector_store = FAISS.from_documents(\n",
        "          self.document_chunks, self.embeddings)\n",
        "      logger.info(\"Created the FAISS vector store.\")\n",
        "\n",
        "  def setup_local_llm(self, model_id=\"google/flan-t5-base\", device=\"auto\"):\n",
        "      tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "      model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=device)\n",
        "      pipe = pipeline(\"text-generation\", model=model,\n",
        "                      tokenizer=tokenizer, max_new_tokens=512, temperature=0.7)\n",
        "      self.llm = HuggingFacePipeline(pipeline=pipe)\n",
        "      logger.info(f\"Local LLM {model_id} ready.\")\n",
        "\n",
        "  # setting up the retrieval qa chain\n",
        "  def setup_qa_chain(self, k=3):\n",
        "      self.qa_chain = RetrievalQA.from_chain_type(\n",
        "          llm=self.llm,\n",
        "          chain_type=\"stuff\",\n",
        "          retriever=self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
        "      )\n",
        "      logger.info(f\"Retrieval QA chain set with top {k} documents retrieved.\")\n",
        "\n",
        "  # answer questions using the RAG system\n",
        "  def answer_question(self, question):\n",
        "      answer = self.qa_chain.run(question)\n",
        "      logger.info(f\"Answered question: {question}\")\n",
        "      return answer\n",
        "\n",
        "  # run the setup\n",
        "  def run_setup(self, chunk_size=1000, chunk_overlap=200, model_id=\"google/flan-t5-base\", k=3):\n",
        "      pdf_paths = self.upload_pdfs()\n",
        "      self.load_documents(pdf_paths)\n",
        "      self.split_documents(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "      self.setup_embeddings()\n",
        "      self.create_vector_store()\n",
        "      self.setup_local_llm(model_id=model_id)\n",
        "      self.setup_qa_chain(k=k)\n",
        "      logger.info(\"RAG summarizer is ready to answer questions.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b893c60"
      },
      "source": [
        "This cell shows us how to use our question-answering robot! It creates an instance of our `LocalRAGSystem`, sets it up with some documents, and then asks two example questions to show how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmhhWigX87xk"
      },
      "outputs": [],
      "source": [
        "# example usage\n",
        "if __name__ == \"__main__\":\n",
        "    rag = LocalRAGSystem()\n",
        "    rag.run_setup()\n",
        "\n",
        "    q1 = \"What is the main topic of these documents?\"\n",
        "    print(f\"Q: {q1}\\nA: {rag.answer_question(q1)}\")\n",
        "\n",
        "    q2 = \"Summarize the key points from the documents in few words and make it easy to understand.\"\n",
        "    print(f\"Q: {q2}\\nA: {rag.answer_question(q2)}\")\n",
        "\n",
        "    q3 = \"What is Modular RAG?\"\n",
        "    print(f\"Q: {q3}\\nA: {rag.answer_question(q3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857853ce"
      },
      "source": [
        "This cell defines a special function that our Gradio web interface will use. When you type a question into the web app, this function will take your question and feed it to our RAG system to get an answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHSsrWPb94FE"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rag_answer_question(question):\n",
        "    if rag is None or rag.qa_chain is None:\n",
        "        return \"RAG system not initialized. Please run the setup first.\"\n",
        "    return rag.answer_question(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "530202cc"
      },
      "source": [
        "This cell brings our question-answering robot to life as a simple website! It creates an interactive interface using Gradio, allowing you to type in questions and see the answers directly in your web browser."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interface = gr.Interface(\n",
        "    fn=rag_answer_question,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "    outputs=gr.Textbox(lines=15), # Increased lines for better output visibility\n",
        "    title=\"Local RAG System Question Answering\",\n",
        "    description=\"Ask a question about the documents loaded into the RAG system.\"\n",
        ")\n",
        "\n",
        "# Set share=True to get a public link accessible outside your Colab session\n",
        "# (This is often default behavior in Colab, but good to be explicit)\n",
        "interface.launch(share=True)"
      ],
      "metadata": {
        "id": "137kRYML2_4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XuUubHZhkK7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMW2CX0mNLhd9l4LRMRItt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}